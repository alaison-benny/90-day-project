# **Diffrent types of terraform files**
**Main	.tf file, Variables	.tf, Output	.tf, State File	.tfstate file, Lock File	.hcl file**
# 1. main.tf (പ്രധാന ഫയൽ)
ഇതാണ് Terraform-ന്റെ ഹൃദയം. നിങ്ങൾക്ക് എന്തൊക്കെ റിസോഴ്‌സുകൾ വേണമെന്ന് ഇതിലാണ് എഴുതുന്നത്.
ഉദാഹരണം: ML മോഡൽ ട്രെയിൻ ചെയ്യാൻ ഒരു AWS SageMaker നോട്ട്ബുക്കോ അല്ലെങ്കിൽ ഒരു EC2 GPU ഇൻസ്റ്റൻസോ വേണമെന്ന് നിങ്ങൾ ഇതിൽ പറയും.        
# 2. variables.tf (വേരിയബിൾ ഫയൽ)
കോഡിനുള്ളിൽ നേരിട്ട് വാല്യൂകൾ നൽകാതെ (Hardcoding), അവയെ മാറ്റി നിർത്താൻ ഇത് സഹായിക്കുന്നു.
ഉദാഹരണം: പരീക്ഷണ ഘട്ടത്തിൽ (Dev) ചെറിയ സെർവറും, ശരിക്കുള്ള ഉപയോഗത്തിന് (Prod) വലിയ സെർവറും വേണമെങ്കിൽ, കോഡ് മാറ്റാതെ ഈ ഫയലിലെ വാല്യൂ മാത്രം മാറ്റിയാൽ മതി.
ML Context: മോഡൽ ട്രെയിനിംഗിന് ആവശ്യമായ instance_type (ഉദാ: p3.2xlarge) ഇവിടെ ഡിഫൈൻ ചെയ്യാം.
# 3. outputs.tf (ഔട്ട്‌പുട്ട് ഫയൽ)
ഇൻഫ്രാസ്ട്രക്ചർ നിർമ്മിച്ച് കഴിഞ്ഞാൽ നമുക്ക് ആവശ്യമായ ചില വിവരങ്ങൾ ഇത് കാണിച്ചുതരും.
ഉദാഹരണം: നിങ്ങൾ ഒരു ML മോഡൽ ഹോസ്റ്റ് ചെയ്യാൻ ഒരു എൻഡ്-പോയിന്റ് ഉണ്ടാക്കി. ആ ലിങ്ക് (URL) എന്താണെന്ന് ഔട്ട്‌പുട്ടിലൂടെ കാണാം.
Use Case: അടുത്ത ഘട്ടത്തിൽ ഡാറ്റാ സയന്റിസ്റ്റിന് ആ ലിങ്ക് കൈമാറാൻ ഇത് സഹായിക്കുന്നു
# 4. terraform.tfstate (സ്റ്റേറ്റ് ഫയൽ)
നിങ്ങൾ terraform apply ചെയ്യുമ്പോൾ ക്ലൗഡിൽ എന്തൊക്കെ മാറ്റങ്ങൾ വന്നു എന്ന് Terraform കുറിച്ചു വെക്കുന്ന ഡയറിയാണിത്. ഇത് ഒരിക്കലും കൈകൊണ്ട് എഡിറ്റ് ചെയ്യാൻ പാടില്ല.
ML Context: ഒരാൾ ഒരു GPU സർവർ നിർമ്മിച്ചു, മറ്റൊരാൾ അത് ഡിലീറ്റ് ചെയ്യാൻ ശ്രമിച്ചാൽ Terraform ഈ ഫയൽ നോക്കി അത് തടയുകയോ അറിയിക്കുകയോ ചെയ്യും.
# നിങ്ങൾ ശ്രദ്ധിക്കേണ്ട പ്രധാന കാര്യം:
ഒരു ML പ്ലാറ്റ്‌ഫോം എൻജിനീയർ എന്ന നിലയിൽ, ഈ ഫയലുകൾ ഒരു Git Repository-ൽ (GitHub/GitLab) സൂക്ഷിക്കണം.
ഒരു പ്രൊഡക്ഷൻ-ഗ്രേഡ് (Production Grade) ജനറേറ്റീവ് AI ആപ്ലിക്കേഷൻ (ChatGPT പോലുള്ളവ) നിർമ്മിക്കുമ്പോൾ, വെറുമൊരു സെർവർ ഉണ്ടാക്കുന്നത് പോലെയല്ല കാര്യങ്ങൾ. ഇതിന് ഉയർന്ന സുരക്ഷയും, പെർഫോമൻസും, ചെലവ് നിയന്ത്രിക്കാനുള്ള സംവിധാനങ്ങളും ആവശ്യമാണ്.

# **ഒരു `main.tf` ഫയലിൽ ഉൾപ്പെടുത്തേണ്ട പ്രധാന കാര്യങ്ങൾ താഴെ പറയുന്നവയാണ്:**

---

### 1. Provider and Backend Configuration (അടിസ്ഥാന സജ്ജീകരണം)

ആദ്യം ഏത് ക്ലൗഡ് (AWS, Azure, GCP) ആണ് ഉപയോഗിക്കുന്നത് എന്ന് വ്യക്തമാക്കണം. പ്രൊഡക്ഷൻ ആയതുകൊണ്ട് തന്നെ `State File` സ്വന്തം കമ്പ്യൂട്ടറിൽ വെക്കാതെ ക്ലൗഡിൽ (S3 Bucket) സൂക്ഷിക്കണം.

* **ഉപയോഗം:** ഒന്നിലധികം എൻജിനീയർമാർക്ക് ഒരേസമയം ജോലി ചെയ്യാനും ഡാറ്റ നഷ്ടപ്പെടാതിരിക്കാനും.

### 2. VPC & Networking (സുരക്ഷിതമായ ശൃംഖല)

ChatGPT പോലുള്ള ആപ്പുകൾക്ക് വലിയ തോതിലുള്ള ഡാറ്റ കൈമാറ്റം ആവശ്യമാണ്. അതിനാൽ ഒരു ക്ലൗഡ് നെറ്റ്‌വർക്ക് (VPC) നിർമ്മിക്കണം.

* **Public Subnet:** പുറംലോകത്തിന് ആപ്പ് കാണാൻ (Load Balancer).
* **Private Subnet:** AI മോഡലുകൾ റൺ ചെയ്യുന്ന സെർവറുകൾ ഇവിടെയായിരിക്കണം. ഇത് പുറത്തുനിന്നുള്ള ഹാക്കിംഗിൽ നിന്ന് സംരക്ഷണം നൽകുന്നു.

### 3. Compute Resources - GPU Instances (കമ്പ്യൂട്ടിംഗ് പവർ)

Generative AI-ക്ക് സാധാരണ CPU പോരാ, NVIDIA A100 അല്ലെങ്കിൽ H100 പോലുള്ള **GPU** സർവറുകൾ വേണം.

* **Auto Scaling Group:** തിരക്കുള്ള സമയത്ത് കൂടുതൽ സർവറുകൾ താനേ വരാനും, തിരക്ക് കുറയുമ്പോൾ അവ ഓഫ് ആകാനും ഇത് സഹായിക്കുന്നു.

### 4. IAM Roles & Security Groups (സുരക്ഷാ കവചം)

ആർക്കൊക്കെ ഡാറ്റ കാണാം, ഏതൊക്കെ പോർട്ടുകൾ തുറന്നിടണം എന്ന് ഇതിൽ തീരുമാനിക്കുന്നു.

* **Principle of Least Privilege:** ആവശ്യത്തിന് മാത്രം അനുവാദം നൽകുക. ഉദാഹരണത്തിന്, AI മോഡലിന് ഡാറ്റ റീഡ് ചെയ്യാൻ മാത്രം അനുവാദം നൽകുക, ഡിലീറ്റ് ചെയ്യാൻ നൽകരുത്.

### 5. Storage for Models (മോഡൽ സൂക്ഷിക്കാൻ)

ഭീമമായ വലിപ്പമുള്ള AI മോഡലുകൾ (Weights) സൂക്ഷിക്കാൻ S3 പോലുള്ള ഒബ്ജക്റ്റ് സ്റ്റോറേജ് വേണം.

* **Versioning:** മോഡലിന്റെ പുതിയ വേർഷൻ വരുമ്പോൾ പഴയത് നഷ്ടപ്പെടാതെ സൂക്ഷിക്കാൻ ഇത് സഹായിക്കുന്നു.

### 6. Monitoring & Logging (നിരീക്ഷണം)

സിസ്റ്റം തകരാറിലായാൽ ഉടൻ അറിയാൻ CloudWatch പോലുള്ള സർവീസുകൾ കണക്ട് ചെയ്യണം.

---

### ഒരു റിയൽ വേൾഡ് കമ്പനി ഉദാഹരണം (Example: OpenAI/Anthropic)

നിങ്ങൾ ഒരു ChatGPT പോലുള്ള ആപ്പ് ലോഞ്ച് ചെയ്യുന്നു എന്ന് കരുതുക:

1. **പ്രശ്നം:** പെട്ടെന്ന് പത്ത് ലക്ഷം ആളുകൾ ഒരേ സമയം ആപ്പ് ഉപയോഗിക്കാൻ വന്നാൽ സിസ്റ്റം ഡൗൺ ആകും.
2. **Terraform പരിഹാരം:** നിങ്ങളുടെ `main.tf`-ൽ ഒരു **Load Balancer** സെറ്റ് ചെയ്യുന്നു. ഇത് വരുന്ന ട്രാഫിക്കിനെ പല സെർവറുകളിലേക്ക് വീതിച്ചു നൽകുന്നു.
3. **ചെലവ് കുറയ്ക്കാൻ:** `main.tf`-ൽ **Spot Instances** ഉപയോഗിക്കാം. ഇത് സാധാരണ സർവറുകളേക്കാൾ 70-90% കുറഞ്ഞ നിരക്കിൽ ലഭിക്കും (നോൺ-ക്രിട്ടിക്കൽ ജോലികൾക്ക്).

---

### ഒരു ലളിതമായ `main.tf` ഘടന (Skeleton)

```hcl
# 1. Provider സെറ്റ് ചെയ്യുന്നു
provider "aws" {
  region = "us-east-1"
}

# 2. VPC നിർമ്മിക്കുന്നു
resource "aws_vpc" "ai_vpc" {
  cidr_block = "10.0.0.0/16"
}

# 3. AI മോഡൽ സൂക്ഷിക്കാനുള്ള സ്റ്റോറേജ്
resource "aws_s3_bucket" "model_storage" {
  bucket = "my-gen-ai-models-prod"
}

# 4. GPU സെർവർ (EC2)
resource "aws_instance" "gpu_server" {
  ami           = "ami-xxxxxx" 
  instance_type = "p3.2xlarge" # GPU ഉള്ള ഇൻസ്റ്റൻസ്
}

```

### നിങ്ങൾ ചെയ്യേണ്ട അടുത്ത പടി:

ഒരു പ്രൊഡക്ഷൻ ഫയലിൽ **Hardcode** ചെയ്യാൻ പാടില്ല (ഉദാഹരണത്തിന് പാസ്‌വേഡുകൾ നേരിട്ട് എഴുതരുത്). 
ഒരു പ്രൊഡക്ഷൻ എൻവയോൺമെന്റിൽ സെക്യൂരിറ്റി വളരെ പ്രധാനമാണ്. `main.tf` ഫയലിലോ മറ്റോ പാസ്‌വേഡുകൾ, API കീകൾ (ഉദാഹരണത്തിന് OpenAI API Key), അല്ലെങ്കിൽ ഡാറ്റാബേസ് പാസ്‌വേഡുകൾ നേരിട്ട് എഴുതുന്നത് വലിയൊരു സുരക്ഷാ വീഴ്ചയാണ്. ഇതിനെ **Hardcoding** എന്ന് വിളിക്കുന്നു.

ഇത് ഒഴിവാക്കാനാണ് **AWS Secrets Manager** അല്ലെങ്കിൽ **HashiCorp Vault** ഉപയോഗിക്കുന്നത്. ഇത് എങ്ങനെയാണ് പ്രവർത്തിക്കുന്നതെന്ന് താഴെ വിവരിക്കാം.

**ഘട്ടം 1: രഹസ്യം ക്ലൗഡിൽ സൂക്ഷിക്കുക**
ആദ്യം നമ്മൾ AWS കൺസോളിൽ പോയി `my-ai-api-key` എന്ന പേരിൽ കീ സേവ് ചെയ്യുന്നു.

**ഘട്ടം 2: Terraform കോഡിൽ ഇത് വിളിക്കുക**
നമ്മുടെ `main.tf`-ൽ താഴെ പറയുന്ന രീതിയിൽ കോഡ് എഴുതാം:

```hcl
# 1. സെക്രെട്സ് മാനേജറിലുള്ള ഡാറ്റ റീഡ് ചെയ്യുന്നു
data "aws_secretsmanager_secret" "ai_key" {
  name = "prod/chatgpt/api_key"
}

data "aws_secretsmanager_secret_version" "current" {
  secret_id = data.aws_secretsmanager_secret.ai_key.id
}

# 2. ഈ കീ ഒരു സെർവറിൽ (EC2) ഉപയോഗിക്കുന്നു
resource "aws_instance" "ai_worker" {
  ami           = "ami-12345"
  instance_type = "p3.2xlarge"

  # കീ നേരിട്ട് നൽകാതെ ഇവിടെ നിന്ന് പാസ്സ് ചെയ്യുന്നു
  user_data = <<EOF
    export OPENAI_API_KEY=${data.aws_secretsmanager_secret_version.current.secret_string}
  EOF
}

```

---

### 3. HashiCorp Vault ഉപയോഗിക്കുന്ന രീതി

ചില കമ്പനികൾ (ഉദാഹരണത്തിന് **JPMorgan** അല്ലെങ്കിൽ വലിയ ടെക് കമ്പനികൾ) എല്ലാ ക്ലൗഡിലും (AWS, Azure, GCP) ഒരേപോലെ പ്രവർത്തിക്കുന്ന **Vault** ആണ് ഉപയോഗിക്കാറുള്ളത്.

* **Dynamic Secrets:** ഇതിന്റെ പ്രത്യേകത എന്താണെന്നുവെച്ചാൽ, ഓരോ തവണ ആവശ്യപ്പെടുമ്പോഴും പുതിയ പാസ്‌വേഡ് ഉണ്ടാക്കി നൽകാനും കുറച്ചു സമയം കഴിഞ്ഞ് അത് താനേ ഡിലീറ്റ് ചെയ്യാനും Vault-ന് സാധിക്കും. ഇത് ഹാക്കിംഗ് സാധ്യത ഇല്ലാതാക്കുന്നു.

---

### നിങ്ങൾ അറിഞ്ഞിരിക്കേണ്ട പ്രധാന കാര്യങ്ങൾ:

* **Rotation:** പാസ്‌വേഡുകൾ കൃത്യമായ ഇടവേളകളിൽ (ഉദാ: 30 ദിവസം) മാറ്റിക്കൊണ്ടിരിക്കാൻ ഈ ടൂളുകൾ സഹായിക്കുന്നു.
* **Audit Logs:** ആരെങ്കിലും പാസ്‌വേഡ് കാണാൻ ശ്രമിച്ചാൽ അതിന്റെ റെക്കോർഡ് ഇതിൽ ഉണ്ടാകും.

# **Terraform Modules**
നമ്മുടെ കയ്യിൽ ഒരു വലിയ ജനറേറ്റീവ് AI (GenAI) ആപ്ലിക്കേഷൻ ഉണ്ടെന്ന് കരുതുക. അതിന് ഡാറ്റാബേസ് വേണം, GPU സർവറുകൾ വേണം, സ്റ്റോറേജ് വേണം. ഓരോ തവണ പുതിയൊരു ആപ്പ് ഉണ്ടാക്കുമ്പോഴും ഈ കോഡ് മുഴുവൻ വീണ്ടും വീണ്ടും എഴുതുന്നത് ബുദ്ധിമുട്ടാണ്.

ഇവിടെയാണ് **Terraform Modules** രക്ഷയ്‌ക്കെത്തുന്നത്.

---

### ഒരു GenAI (ChatGPT) പ്രോജക്റ്റിലെ ഉദാഹരണം

നിങ്ങളുടെ കമ്പനിയിൽ രണ്ട് ടീമുകൾ ഉണ്ടെന്ന് കരുതുക:

1. **Chat-Team:** ചാറ്റിംഗിനുള്ള ആപ്പ് നോക്കുന്നു.
2. **Image-Team:** ഇമേജ് ജനറേഷൻ (DALL-E പോലെ) നോക്കുന്നു.

രണ്ട് ടീമിനും മോഡലുകൾ സൂക്ഷിക്കാൻ **S3 Bucket** വേണം, മോഡൽ റൺ ചെയ്യാൻ **GPU സർവറുകൾ** വേണം. ഇതിനായി നമ്മൾ ഒരു "AI-Infrastructure Module" ഉണ്ടാക്കുന്നു.

#### Module-ന്റെ ഘടന:

ഒരു ഫോൾഡറിനുള്ളിൽ (ഉദാ: `modules/ai-app/`) ഈ ഫയലുകൾ ഉണ്ടാകും:

* `main.tf`: GPU സർവറും സ്റ്റോറേജും നിർമ്മിക്കാനുള്ള കോഡ്.
* `variables.tf`: സർവർ സൈസ് എത്ര വേണം എന്ന് തീരുമാനിക്കാനുള്ള വേരിയബിൾസ്.
* `outputs.tf`: സർവറിന്റെ ലിങ്ക് പുറത്തേക്ക് നൽകാൻ.

---

### ഇത് എങ്ങനെ ഉപയോഗിക്കാം? (Real-World Implementation)

നിങ്ങളുടെ മെയിൻ പ്രോജക്റ്റിൽ ഈ മോഡ്യൂളിനെ താഴെ പറയുന്ന രീതിയിൽ വിളിക്കാം:

```hcl
# ChatGPT-ക്ക് വേണ്ടിയുള്ള ഇൻഫ്രാസ്ട്രക്ചർ
module "chatgpt_infra" {
  source        = "./modules/ai-app"
  instance_type = "p3.2xlarge"  # വലിയ GPU
  app_name      = "chat-bot"
}

# Image Generation-ന് വേണ്ടിയുള്ള ഇൻഫ്രാസ്ട്രക്ചർ
module "image_gen_infra" {
  source        = "./modules/ai-app"
  instance_type = "p3.8xlarge"  # അതിലും വലിയ GPU വേണം
  app_name      = "image-creator"
}

```

ഇവിടെ ശ്രദ്ധിച്ചാൽ മനസ്സിലാകും, ഒരേ മോഡ്യൂൾ ഉപയോഗിച്ച് രണ്ട് വ്യത്യസ്ത തരം ആപ്പുകൾക്ക് വേണ്ട ഇൻഫ്രാസ്ട്രക്ചർ നമ്മൾ നിർമ്മിച്ചു.

---

### കമ്പനികളിലെ ഉപയോഗം 

1. **പ്രശ്നം:** ഓരോ ടീമും അവരുടേതായ രീതിയിൽ സർവറുകൾ ഉണ്ടാക്കിയാൽ അത് സുരക്ഷയെ ബാധിക്കാം, ചെലവ് കൂടാം.
2. **പരിഹാരം:** കമ്പനിയിലെ 'Platform Engineering' ടീം ഒരു സ്റ്റാൻഡേർഡ് Terraform Module ഉണ്ടാക്കി നൽകും.
3. **ഗുണം:** ബാക്കി ടീമുകൾക്ക് ആ മോഡ്യൂൾ ഉപയോഗിച്ച് മിനിറ്റുകൾക്കുള്ളിൽ സുരക്ഷിതമായ സർവറുകൾ റെഡിയാക്കാം.

---

### ഒരു ML Platform Engineer ശ്രദ്ധിക്കേണ്ട പ്രധാന കാര്യങ്ങൾ:

* **Version Control:** മോഡ്യൂളുകൾ മാറ്റുമ്പോൾ വേർഷനുകൾ (v1.0, v2.0) നൽകണം. അല്ലെങ്കിൽ ഒരു ടീം വരുത്തുന്ന മാറ്റം മറ്റൊരാളുടെ ആപ്പിനെ ബാധിച്ചേക്കാം.
* **Documentation:** മോഡ്യൂൾ എങ്ങനെ ഉപയോഗിക്കണം എന്ന് വ്യക്തമായ കുറിപ്പുകൾ ഉണ്ടായിരിക്കണം.

### Terraform കമാൻഡ്
Init	പ്ലഗിനുകൾ ലോഡ്, ചെയ്യുന്നു	AWS സെറ്റപ്പ് ചെയ്യുന്നു	
Plan	മാറ്റങ്ങൾ കണക്കുകൂട്ടുന്നു,	"1 GPU സർവർ വേണം" എന്ന് കണ്ടെത്തുന്നു	
Apply	റിസോഴ്സ് നിർമ്മിക്കുന്നു,	GPU സർവർ ലൈവ് ആകുന്നു	
Destroy	റിസോഴ്സ് നീക്കം ചെയ്യുന്നു,	സർവർ ഓഫ് ചെയ്ത് പണം ലാഭിക്കുന്നു	

### ഒരു ML പ്ലാറ്റ്‌ഫോം എൻജിനീയർ എന്ന നിലയിൽ നിങ്ങൾ ശ്രദ്ധിക്കേണ്ടത്:
നിങ്ങൾ പ്രൊഡക്ഷനിൽ apply ചെയ്യുമ്പോൾ അത് വളരെ ശ്രദ്ധിക്കണം. കാരണം ഒരു ചെറിയ തെറ്റ് വന്നാൽ നിലവിലുള്ള ChatGPT സർവർ ഡൗൺ ആയേക്കാം. അതുകൊണ്ട് എപ്പോഴും plan നന്നായി പരിശോധിച്ച ശേഷം മാത്രമേ apply ചെയ്യാൻ പാടുള്ളൂ.
## Terraform State
Terraform എങ്ങനെയാണ് കാര്യങ്ങൾ ഓർത്തു വെക്കുന്നത് എന്നും അത് എങ്ങനെയാണ് സുരക്ഷിതമായി കൈകാര്യം ചെയ്യേണ്ടത് എന്നും താഴെ വിവരിക്കാം.

---

### എന്താണ് Terraform State? (The Theory)

ലളിതമായി പറഞ്ഞാൽ, Terraform നിർമ്മിച്ച എല്ലാ കാര്യങ്ങളുടെയും ഒരു **റെക്കോർഡ് ബുക്ക്** അല്ലെങ്കിൽ **മാപ്പ്** ആണിത്.

* നിങ്ങൾ കോഡ് എഴുതിക്കഴിഞ്ഞാൽ, ക്ലൗഡിൽ (AWS/GCP) എന്തൊക്കെ മാറ്റങ്ങൾ വരുത്തി എന്ന് Terraform ഒരു JSON ഫയലിൽ കുറിച്ചു വെക്കും. ഇതാണ് `terraform.tfstate`.
* അടുത്ത തവണ നിങ്ങൾ കോഡിൽ മാറ്റം വരുത്തുമ്പോൾ, Terraform ഈ ഫയൽ നോക്കിയാണ് "ഓ, ഞാൻ നേരത്തെ ഒരു GPU സർവർ ഉണ്ടാക്കിയിട്ടുണ്ടല്ലോ, ഇനി പുതിയൊരു സ്റ്റോറേജ് കൂടി ഉണ്ടാക്കിയാൽ മതി" എന്ന് മനസ്സിലാക്കുന്നത്.

---

### ChatGPT ഉദാഹരണത്തിലൂടെയുള്ള വിശദീകരണം

നിങ്ങൾ ChatGPT-ക്ക് വേണ്ടി ഒരു ഇൻഫ്രാസ്ട്രക്ചർ നിർമ്മിക്കുകയാണെന്ന് കരുതുക:

1. **ആദ്യത്തെ അവസ്ഥ (Initial State):** നിങ്ങൾ `main.tf`-ൽ ഒരു NVIDIA GPU സർവർ വേണമെന്ന് എഴുതി `apply` ചെയ്യുന്നു. Terraform ക്ലൗഡിൽ ആ സർവർ ഉണ്ടാക്കുന്നു. അതിന്റെ ID (ഉദാ: `i-0abc123`) സ്റ്റേറ്റ് ഫയലിൽ രേഖപ്പെടുത്തുന്നു.
2. **മാറ്റം വരുത്തുമ്പോൾ (Update):** അടുത്ത ദിവസം നിങ്ങൾ വിചാരിക്കുന്നു, "സർവർ മാത്രം പോരാ, ഒരു S3 ബക്കറ്റ് കൂടി വേണം". നിങ്ങൾ കോഡിൽ അത് ചേർക്കുന്നു. `plan` ചെയ്യുമ്പോൾ Terraform ആദ്യം സ്റ്റേറ്റ് ഫയൽ നോക്കും.
* **Terraform ചിന്തിക്കുന്നത്:** "സ്റ്റേറ്റ് ഫയൽ പ്രകാരം `i-0abc123` എന്ന സർവർ നിലവിലുണ്ട്. പുതിയതായി ഒരു ബക്കറ്റ് മാത്രമേ ഉണ്ടാക്കേണ്ടതുള്ളൂ."


3. **ഒറ്റയ്ക്ക് മാറ്റം വരുത്തിയാൽ (Drift):** നിങ്ങൾ Terraform അറിയാതെ നേരിട്ട് AWS കൺസോളിൽ പോയി ആ സർവർ ഡിലീറ്റ് ചെയ്തു എന്ന് കരുതുക. അടുത്ത തവണ `terraform plan` ചെയ്യുമ്പോൾ, സ്റ്റേറ്റ് ഫയലും ക്ലൗഡിലെ അവസ്ഥയും തമ്മിൽ വ്യത്യാസം (Drift) ഉണ്ടെന്ന് Terraform കണ്ടെത്തും. അത് ഉടൻ തന്നെ ആ സർവർ വീണ്ടും നിർമ്മിക്കാൻ ശ്രമിക്കും.

---

### State Management-ലെ പ്രധാന കാര്യങ്ങൾ

ഒരു ML Platform Engineer എന്ന നിലയിൽ നിങ്ങൾ അറിഞ്ഞിരിക്കേണ്ട മൂന്ന് കാര്യങ്ങൾ:

#### 1. Local vs Remote State

* **Local State:** സ്റ്റേറ്റ് ഫയൽ നിങ്ങളുടെ കമ്പ്യൂട്ടറിൽ മാത്രം ഇരിക്കുന്ന അവസ്ഥ. ഇത് അപകടമാണ്. കാരണം നിങ്ങളുടെ കമ്പ്യൂട്ടർ കേടായാൽ ഇൻഫ്രാസ്ട്രക്ചറിന്റെ നിയന്ത്രണം നഷ്ടപ്പെടും.
* **Remote State:** സ്റ്റേറ്റ് ഫയൽ സുരക്ഷിതമായ ഒരു ക്ലൗഡ് സ്റ്റോറേജിൽ (AWS S3 അല്ലെങ്കിൽ Google Cloud Storage) സൂക്ഷിക്കുന്നു. ഇതാണ് പ്രൊഡക്ഷനിൽ ഉപയോഗിക്കുന്നത്.

#### 2. State Locking

ഒരേ സമയം രണ്ട് എൻജിനീയർമാർ ChatGPT-യുടെ കോഡിൽ മാറ്റം വരുത്താൻ ശ്രമിച്ചാൽ എന്ത് സംഭവിക്കും?

* രണ്ടുപേരും ഒരേ സമയം `apply` ചെയ്താൽ സ്റ്റേറ്റ് ഫയൽ തകരാറിലാകാൻ (Corruption) സാധ്യതയുണ്ട്.
* ഇത് ഒഴിവാക്കാൻ **State Locking** ഉപയോഗിക്കുന്നു (ഉദാഹരണത്തിന് DynamoDB ഉപയോഗിച്ച്). ഒരാൾ ജോലി ചെയ്യുമ്പോൾ മറ്റൊരാൾക്ക് മാറ്റം വരുത്താൻ സാധിക്കില്ല (ലോക്ക് ചെയ്യും).

#### 3. Security (Secrets in State)

സ്റ്റേറ്റ് ഫയലിൽ നിങ്ങളുടെ ഡാറ്റാബേസ് പാസ്‌വേഡുകളോ API കീകളോ പ്ലെയിൻ ടെക്സ്റ്റ് ആയി ഉണ്ടാകാൻ സാധ്യതയുണ്ട്. അതിനാൽ ഈ ഫയൽ ആർക്കും കാണാൻ കഴിയാത്ത വിധം സുരക്ഷിതമായി എൻക്രിപ്റ്റ് ചെയ്ത് വെക്കണം.

---

### ചുരുക്കത്തിൽ:

സ്റ്റേറ്റ് ഫയൽ എന്നത് Terraform-ന്റെ **ഓർമ്മശക്തിയാണ്**. ഇത് നഷ്ടപ്പെട്ടാൽ നിങ്ങളുടെ ക്ലൗഡ് റിസോഴ്‌സുകളെ നിയന്ത്രിക്കാൻ Terraform-ന് കഴിയില്ല.

# State Conflict
Terraform ഉപയോഗിച്ച് ഒരു Generative AI ആപ്ലിക്കേഷൻ (ChatGPT പോലുള്ളവ) വലിയൊരു ടീം ചേർന്ന് നിർമ്മിക്കുമ്പോൾ നേരിടുന്ന ഏറ്റവും വലിയ വെല്ലുവിളിയാണ് **'State Conflict'**.

ലളിതമായി പറഞ്ഞാൽ, ഒരേ സമയം ഒന്നിലധികം ആളുകൾ ഒരേ ഇൻഫ്രാസ്ട്രക്ചറിൽ മാറ്റങ്ങൾ വരുത്താൻ ശ്രമിക്കുമ്പോൾ ഉണ്ടാകുന്ന ആശയക്കുഴപ്പമാണിത്. ഇത് എങ്ങനെ ഒഴിവാക്കാം എന്ന് താഴെ വിവരിക്കുന്നു.

---

### എന്താണ് ഈ 'Conflict'? (The Problem)

നിങ്ങളുടെ കമ്പനിയിൽ രണ്ട് ML പ്ലാറ്റ്‌ഫോം എൻജിനീയർമാർ ഉണ്ടെന്ന് കരുതുക: **അക്ഷയ്** ഉം **അനുവും**.

1. **അക്ഷയ്:** ChatGPT-യുടെ സർവർ കപ്പാസിറ്റി കൂട്ടാൻ തീരുമാനിക്കുന്നു (ഉദാഹരണത്തിന് 2 GPU-വിൽ നിന്ന് 5 GPU ആക്കാൻ). അക്ഷയ് `terraform apply` കമാൻഡ് റൺ ചെയ്യുന്നു.
2. **അനു:** ഇതേ സമയം തന്നെ ChatGPT-യുടെ ഡാറ്റാബേസ് പാസ്‌വേഡ് മാറ്റാൻ ശ്രമിക്കുന്നു. അനുവും `terraform apply` റൺ ചെയ്യുന്നു.

രണ്ടുപേരും ഒരേ സമയം ഒരേ **State File** അപ്‌ലോഡ് ചെയ്യാൻ ശ്രമിച്ചാൽ, ആ ഫയൽ തകരാറിലാകുകയോ (Corrupt), ഒരാൾ വരുത്തിയ മാറ്റം മറ്റൊരാൾ അറിയാതെ മായ്ച്ചു കളയുകയോ (Overwrite) ചെയ്തേക്കാം. ഇതിനെയാണ് Conflict എന്ന് വിളിക്കുന്നത്.

---

### ഇത് എങ്ങനെ ഒഴിവാക്കാം? (The Solutions)

പ്രൊഡക്ഷൻ എൻവയോൺമെന്റുകളിൽ ഈ പ്രശ്നം ഒഴിവാക്കാൻ പ്രധാനമായും മൂന്ന് വഴികളാണുള്ളത്:

#### 1. Remote State Locking (ലോക്കിംഗ് സിസ്റ്റം)

ഇതാണ് ഏറ്റവും ഫലപ്രദമായ വഴി. ഒരു എൻജിനീയർ പ്ലാൻ അല്ലെങ്കിൽ അപ്ലൈ ചെയ്യുമ്പോൾ, Terraform ആ സ്റ്റേറ്റ് ഫയലിനെ 'ലോക്ക്' ചെയ്യും.

* **എങ്ങനെ പ്രവർത്തിക്കുന്നു:** സാധാരണയായി AWS ഉപയോഗിക്കുന്നവർ **DynamoDB** എന്ന ഡാറ്റാബേസ് ഉപയോഗിച്ചാണ് ഇത് ചെയ്യുന്നത്.
* **ChatGPT ഉദാഹരണം:** അക്ഷയ് `apply` ചെയ്യുമ്പോൾ Terraform ഉടൻ തന്നെ DynamoDB-യിൽ ഒരു ലോക്ക് ഇടും. ആ സമയം അനു `apply` ചെയ്യാൻ ശ്രമിച്ചാൽ "State is locked by Akshay" എന്ന മെസ്സേജ് വരികയും അനുവിന് കാത്തിരിക്കേണ്ടി വരികയും ചെയ്യും.

#### 2. Remote Backend (ശരിയായ സൂക്ഷിപ്പുകാരൻ)

സ്റ്റേറ്റ് ഫയൽ സ്വന്തം കമ്പ്യൂട്ടറിൽ വെക്കാതെ **AWS S3** പോലെയുള്ള ഒരു സെൻട്രൽ സ്റ്റോറേജിൽ വെക്കുക.

* **ഗുണം:** എല്ലാവരും ഒരേ ഫയലിലേക്കാണ് നോക്കുന്നത് എന്ന് ഇത് ഉറപ്പാക്കുന്നു. അക്ഷയ് വരുത്തിയ മാറ്റം ഉടൻ തന്നെ അനുവിനും കാണാൻ സാധിക്കും.

#### 3. State Splitting & Workspaces (വിഭജിച്ചു ഭരിക്കുക)

മുഴുവൻ ഇൻഫ്രാസ്ട്രക്ചറും ഒരേ സ്റ്റേറ്റ് ഫയലിൽ വെക്കാതെ ചെറിയ ഭാഗങ്ങളായി തിരിക്കുക.

* **ChatGPT ഉദാഹരണം:** 'Networking' ഒരു ഫയലിലും, 'AI Model Servers' മറ്റൊരു ഫയലിലും, 'Database' വേറൊന്നിലും സൂക്ഷിക്കുക.
* ഇതിലൂടെ അക്ഷയ് മോഡൽ സെർവറിൽ മാറ്റം വരുത്തുമ്പോൾ അനുവിന് നെറ്റ്‌വർക്കിംഗ് ഫയലിൽ പ്രശ്നമില്ലാതെ ജോലി ചെയ്യാം.

---

### കമ്പനികളിലെ ഉപയോഗം (Real-World Example)

**OpenAI** പോലുള്ള കമ്പനികളിൽ നൂറുകണക്കിന് എൻജിനീയർമാർ ഒരേ സമയം ജോലി ചെയ്യുന്നുണ്ടാകും.

1. അവർ നേരിട്ട് `terraform apply` ചെയ്യാറില്ല.
2. പകരം അവർ കോഡ് **GitHub**-ലേക്ക് അപ്‌ലോഡ് ചെയ്യും.
3. **CI/CD Pipeline** (ഉദാഹരണത്തിന് Terraform Cloud അല്ലെങ്കിൽ GitHub Actions) വഴി മാത്രമേ മാറ്റങ്ങൾ നടപ്പിലാക്കൂ.
4. പൈപ്പ്‌ലൈൻ തന്നെ ലോക്കിംഗ് സിസ്റ്റം കൈകാര്യം ചെയ്യുന്നതുകൊണ്ട് രണ്ട് മനുഷ്യർ തമ്മിൽ കോൺഫ്ലിക്റ്റ് ഉണ്ടാകാനുള്ള സാധ്യത ഇല്ലാതാകുന്നു.

---

### ഒരു ML പ്ലാറ്റ്‌ഫോം എൻജിനീയർക്കുള്ള ടിപ്സ്:

* എപ്പോഴും `terraform plan` നോക്കി ആരെങ്കിലും ലോക്ക് ചെയ്തിട്ടുണ്ടോ എന്ന് പരിശോധിക്കുക.
* വലിയ ടീമാണെങ്കിൽ **Terraform Cloud** അല്ലെങ്കിൽ **Spacelift** പോലുള്ള ടൂളുകൾ ഉപയോഗിക്കുക, ഇവ കോൺഫ്ലിക്റ്റുകൾ ഒഴിവാക്കാൻ പ്രത്യേകം നിർമ്മിക്കപ്പെട്ടവയാണ്.

  ഒരു Generative AI ആപ്ലിക്കേഷന്റെ (ChatGPT പോലുള്ളവ) ML പൈപ്പ്‌ലൈൻ എങ്ങനെ Terraform വഴി ഓട്ടോമേറ്റ് ചെയ്യാം എന്ന് താഴെ വിവരിക്കുന്നു. ഇവിടെ ഒരു കാര്യം ശ്രദ്ധിക്കണം: Terraform മോഡൽ ട്രെയിൻ ചെയ്യില്ല, പകരം മോഡൽ ട്രെയിൻ ചെയ്യാൻ ആവശ്യമായ **ഇൻഫ്രാസ്ട്രക്ചർ** കൃത്യമായ ക്രമത്തിൽ ഒരുക്കി നൽകുകയാണ് ചെയ്യുന്നത്.

ഒരു ML പൈപ്പ്‌ലൈനിലെ പ്രധാന ഘട്ടങ്ങളും അത് Terraform എങ്ങനെ കൈകാര്യം ചെയ്യുന്നു എന്നും നോക്കാം.

---
# ഒരു Generative AI ആപ്ലിക്കേഷന്റെ (ChatGPT പോലുള്ളവ) ML പൈപ്പ്‌ലൈൻ എങ്ങനെ Terraform വഴി ഓട്ടോമേറ്റ് ചെയ്യാം എന്ന് താഴെ വിവരിക്കുന്നു
### 1. Data Loading Stage (ഡാറ്റ തയ്യാറാക്കൽ)

ChatGPT പോലുള്ള മോഡലുകൾക്ക് ഭീമമായ ഡാറ്റ ആവശ്യമാണ്. ഈ ഡാറ്റ സൂക്ഷിക്കാനുള്ള സ്ഥലം Terraform ഒരുക്കുന്നു.

* **Terraform ചെയ്യുന്നത്:** ഒരു **S3 Bucket** (AWS) അല്ലെങ്കിൽ **Cloud Storage** (GCP) നിർമ്മിക്കുന്നു.
* **ChatGPT ഉദാഹരണം:** മോഡലിന് പഠിക്കാൻ ആവശ്യമായ ടെക്സ്റ്റ് ഫയലുകൾ സൂക്ഷിക്കാൻ `chatgpt-raw-data` എന്ന പേരിൽ ഒരു ബക്കറ്റ് Terraform കോഡ് വഴി ഉണ്ടാക്കുന്നു. ഇതിന് ആവശ്യമായ 'Permissions' (IAM Roles) നൽകുന്നതും Terraform ആണ്.

### 2. Training Stage (ട്രെയിനിംഗ് ഘട്ടം)

ഡാറ്റ റെഡിയായാൽ മോഡൽ ട്രെയിൻ ചെയ്യാൻ വലിയ കമ്പ്യൂട്ടിംഗ് പവർ വേണം.

* **Terraform ചെയ്യുന്നത്:** **AWS SageMaker** അല്ലെങ്കിൽ **Google Vertex AI** പോലുള്ള സർവീസുകൾ സെറ്റ് ചെയ്യുന്നു. ട്രെയിനിംഗിന് ആവശ്യമായ GPU ഇൻസ്റ്റൻസുകൾ (ഉദാ: `p4d.24xlarge`) കൃത്യസമയത്ത് ലഭ്യമാക്കുന്നു.
* **ഓട്ടോമേഷൻ:** ട്രെയിനിംഗ് കഴിയുമ്പോൾ ഈ സർവറുകൾ താനേ ഓഫാകാൻ പാകത്തിന് (Spot Instances) കോഡ് സെറ്റ് ചെയ്യാം. ഇത് വലിയ തുക ലാഭിക്കാൻ സഹായിക്കും.

### 3. Model Deployment / Inference (മോഡൽ പുറത്തിറക്കൽ)

ട്രെയിനിംഗ് കഴിഞ്ഞാൽ ആ മോഡൽ ആളുകൾക്ക് ഉപയോഗിക്കാൻ പാകത്തിന് ഒരു എൻഡ്-പോയിന്റ് (API) ആയി മാറ്റണം.

* **Terraform ചെയ്യുന്നത്:** 1.  ട്രെയിൻ ചെയ്ത മോഡൽ സൂക്ഷിക്കാൻ ഒരു 'Model Registry' ഉണ്ടാക്കുന്നു.
2.  ഒരു **Load Balancer** സെറ്റ് ചെയ്യുന്നു (ലക്ഷക്കണക്കിന് ആളുകൾ ഒരേസമയം ചാറ്റ് ചെയ്യുമ്പോൾ അത് താങ്ങാൻ).
3.  **Kubernetes (EKS)** അല്ലെങ്കിൽ **Lambda** ഫംഗ്ഷനുകൾ വഴി API ലൈവ് ആക്കുന്നു.

---

### ഒരു റിയൽ വേൾഡ് പ്രോജക്റ്റ് ഉദാഹരണം

നിങ്ങൾ ഒരു കമ്പനിയിൽ ജോലി ചെയ്യുകയാണെന്ന് കരുതുക. അവിടെ ഓരോ ആഴ്ചയും പുതിയ ഡാറ്റ ഉപയോഗിച്ച് ChatGPT മോഡൽ അപ്ഡേറ്റ് ചെയ്യണം.

1. **Step 1:** നിങ്ങൾ `terraform apply` റൺ ചെയ്യുന്നു.
2. **Step 2:** Terraform ഉടൻ തന്നെ ഡാറ്റ ലോഡ് ചെയ്യാനുള്ള ക്ലസ്റ്ററും ട്രെയിനിംഗിനുള്ള GPU സെർവറുകളും റെഡിയാക്കുന്നു.
3. **Step 3:** ട്രെയിനിംഗ് കഴിഞ്ഞാൽ, പുതിയ മോഡൽ പഴയതിന് പകരം ഡിപ്ലോയ് ചെയ്യാനുള്ള ഇൻഫ്രാസ്ട്രക്ചർ (Blue-Green Deployment) Terraform താനേ മാറ്റുന്നു.
4. **Step 4:** പണി കഴിഞ്ഞാൽ അധികമുള്ള സർവറുകൾ ഡിലീറ്റ് ചെയ്ത് സിസ്റ്റം ക്ലീൻ ആക്കുന്നു.

---

### Terraform കോഡ് എങ്ങനെയിരിക്കും? (ലളിതമായ രൂപം)

```hcl
# 1. ഡാറ്റാ സ്റ്റോറേജ്
resource "aws_s3_bucket" "ml_data" {
  bucket = "chatgpt-training-data-prod"
}

# 2. ട്രെയിനിംഗ് പ്ലാറ്റ്‌ഫോം (SageMaker)
resource "aws_sagemaker_model" "chatgpt_model" {
  name               = "chatgpt-v2-model"
  execution_role_arn = aws_iam_role.sagemaker_role.arn
  # മോഡൽ ഇരിക്കുന്ന സ്ഥലം
  primary_container {
    image = "1234567890.dkr.ecr.us-east-1.amazonaws.com/chatgpt-repo:latest"
  }
}

# 3. ആളുകൾക്ക് ഉപയോഗിക്കാൻ വേണ്ട API എൻഡ്-പോയിന്റ്
resource "aws_sagemaker_endpoint_configuration" "ec" {
  name = "chatgpt-endpoint-config"
  production_variants {
    variant_name           = "variant-1"
    model_name             = aws_sagemaker_model.chatgpt_model.name
    initial_instance_count = 2
    instance_type          = "ml.g4dn.xlarge"
  }
}

```

---

### ഒരു ML Platform Engineer എന്ന നിലയിൽ ഇതിന്റെ ഗുണം:

* **Consistency:** ഓരോ തവണയും ഒരേപോലെ തന്നെ ഇൻഫ്രാസ്ട്രക്ചർ ലഭിക്കുന്നു. തെറ്റുകൾ വരാനുള്ള സാധ്യത കുറയുന്നു.
* **Scalability:** ചാറ്റ് ചെയ്യുന്ന ആളുകളുടെ എണ്ണം കൂടിയാൽ സെർവറുകളുടെ എണ്ണം കൂട്ടാൻ Terraform കോഡിലെ ഒരു നമ്പർ മാറ്റിയാൽ മതി.
* **Version Control:** കഴിഞ്ഞ മാസം ഉപയോഗിച്ച അതേ ട്രെയിനിംഗ് സെറ്റപ്പ് വേണമെങ്കിൽ പഴയ കോഡ് എടുത്താൽ മതി.


